---
layout: post
title: Thoughts on Spark Summit West 2016
---

*Originally posted on the TAP Community June 13th, 2016, with minor modifications to archive here*

# "Welcome to TAP! How Was Spark Summit?"
 
There is no better way to kick off a role in community management than to immediately attend a conference on one of the underlying technologies. Last week I attended Spark Summit West in San Francisco.
 
Typically when I attend a conference or event I try to do a recap based on each speaker or each session I attend, think "Live Tweeting" but in blog form. I want to share my thoughts and opinions in a play-by-play so you get the benefit of all the knowledge shared without actually having to attend yourself. When I'm coming into a new space for the first time, this style may just offer more confusion than anything else, so I've changed it up a bit. I have separate my thoughts into 3 sections: overall thoughts, keynotes & sessions (notable standout comments and summaries), and further research topics. I have an enterprise web application development background and a ton of Biology classes on my transcript, but this was as much an introduction to Big Data for me as it was an introduction to Spark and similar technology. In addition I only saw a handful of information on TAP, so the phrase "flying blind" was extremely relevant. I am pretty impressed with the amount of information I retained!
 
## Overall Thoughts
I did not attend the Monday tutorial sessions, so I cannot speak for how those went. But I did attend the keynotes and sessions on Tuesday and Wednesday. Almost immediately I noticed that this conference was the most diverse I have ever attended, technical or non-technical. Typically I am the only woman in the room, even when the rooms are the size of ballrooms, but this time it was not the case. It was very refreshing to see.
 
Spark applications contain a lot of pieces. With an open source based structure, those pieces become very swap-able and it sounds like the community wants this competitiveness to continue. Of course there are also bigger companies and industry players like Microsoft and IBM offering Azure and Bluemix resources for Spark applications as well. TAP fits in here quite nicely.
 
Most of the sessions were over my head. At one point I had a running list of items to look up and research later, I've included those in list form in the further research section.
 
Women in Big Data did a lunch session and opened it up to the public. There were two speakers from MapR who spoke on their experiences in tech as women. Both were great and at their core their messages were simple - there is still a lot of work to be done, so do what it takes to support and empower yourself and the other women in the room. They were very inspiring and I recommend joining the Women in Big Data meetup. They are looking to expand their presence with more meetups in more areas.
 
Finally Spark streaming and streaming applications were the hot topics of the conference. The idea that data can be processed and sent to an application in real time means you get everything... right now! There is no need for static sets of batch data, you just get the data when it happens. This is incredibly powerful, particularly in the analytics space. You are able to see trends as they happen and even adjust and retrain models on the fly.
 
## Sessions and Keynotes
There were some notable standouts and key takeaways in a number of sessions I attended. Again, Big Data and analytics are totally new to me.
 
The very first keynote of the conference was by the Databricks team demonstrating the power of their community edition tools. They include short classes, pre-made datasets, and tutorials all available for free. Databricks is looking to bring content previously only available in academia into MOOCs (massive online open courses) and similarly digestible online material.
 
Two developers from Uber talked about their Spark Uber Development kit (aka UDK). They mentioned something that became an overarching theme throughout the rest of the conference - Spark applications are not easy to make. Uber wanted to enable their developers to make Spark jobs easier to create, monitor, debug, and optimize. The more tooling they have available and can standardize on, the less overhead for developers regardless of their experience level.
 
A quick side bar - I'm a StitchFix customer/user. StitchFix is a clothing subscription box service that markets itself as a doorstep personal stylist, my words not theirs. StitchFix has 70+ data scientists, more data scientists than engineers. They had a platform engineer speak on a tool he created called Morticia (as in Morticia Addams of the Addams Family). I won't go into detail on Morticia since the tool is not available publicly, yet, but it helped their data scientists get what they need easily from tooling that was traditionally not designed for them. It's a blending of the data scientist and platform engineering world so both parties see the information they need.
 
One of my favorite sessions was by Riot Games. They use Big Data to suggest character skins, determine if ISP latency/lag is causing gamers issues, and to collect game time data to see if characters are "overpowered". All of this data helps Riot Games mold the gamer experience into the best one possible.
 
Capital One had a keynote on fraud detection and monitoring. They are using visualization tools to determine threat and risk analysis. As you open credit cards or bank accounts, they are able to map the information based on commonalities like address, cell phone number, email, etc. If the same cell phone number was used in both a real and fraudulent account, they looked at any relevant information for finding the fraudster and how future accounts could be strengthened security wise. Seeing a network mapping of bank activities was both exciting and disturbing at how easy it is to see rogue activity.
 
Finally Cloudera hosted a session on 'Top 5 Mistakes When Writing Spark Applications'. Just like any application, if you write poorly optimized code, your performance will suffer. At scale, things will just fail. The talk ended with one of the speakers commenting that "writing Spark applications is an art and not a science. Like Tinder - we keep swiping right but we still aren't sure what we think is going to happen but its fun so we keep doing it."
 
## Further Research
I collected a number of ideas and technology to go back a do some more reading on later. I definitely plan on diving into the Databricks Community Edition classes and information to see what that offers.
 
### Topics for further research/Googling -
* Databricks
* TensorFlow
* Netflix - Meson
* ETL
* YARN
* Skew in data (statistics)
* Spark (additional libraries)
* Streaming
* Azure
* Bluemix
* Hadoop
* Amazon S3
 
## Closing Thoughts
It is not just hype, this is a very exciting space to be in right now. The potential to impact so many lives in a positive manner is incredible. With analytics we can change the world for the better. We can give sight through glasses and phone applications to the blind or just make our user experience a little more personal when we are looking for movies. I am incredibly grateful for this experience and I cannot wait to see what exciting things people do with TAP. Spark Summit was a great way to introduce me to TAP and the surrounding ecosystem that makes up the Big Data and analytics space.
